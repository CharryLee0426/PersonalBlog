{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build A Neural Network From Scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch MNIST Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to get MNIST dataset, which is very classic for image classification. You can fetch it through sklearn, keras, tensorflow, and pytorch. Here we use .csv file from Google Colab. Every time you connect to a Google Colab environment, you will see that there is a folder called 'sample data'. Open it, and you will see a file called 'mnist_train_small.csv'. This is the file we need. We can use pandas to read it. Of course, you can fetch data from other sources. It's up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/mnist_train_small.csv')\n",
    "df2 = pd.read_csv('./data/mnist_test.csv')\n",
    "traindata = np.array(df)\n",
    "testdata = np.array(df2)\n",
    "m = traindata.shape[0] + testdata.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29998"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into Training set and Testing set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, google helped you to do this split task. However, if noone helps you, you can do it by yourself. It's very easy. Just use `numpy.random.shuffle` to shuffle the dataset to make the whole dataset more balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = traindata.T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19999\n"
     ]
    }
   ],
   "source": [
    "print(m_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset is a `19,999x785` matrix. That means we have 19,999 handwritten digits, and the shape of each digit is `78x78`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = testdata.T\n",
    "Y_test = data_test[0]\n",
    "X_test = data_test[1:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Architecture of Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of this neural network is very simple. It has three baisc layers: input, hidden, and output. Theroetically, the more hidden layers the network has, the more accurate the network will be. However, it is very complicated if there are many layers without using any deep learning framework. So we just use one hidden layer here.\n",
    "\n",
    "In input layer, there are 784 neurons. It's easy to understand because every pixel in digit is an input signal. In hidden layer, there are 10 neurons. These 10 neurons can extract some features in train dataset. In output layer, there are 10 neurons. Because our handwritten digits are labeld from 0 to 9, one neuron represents one label, or one type of digit.\n",
    "\n",
    "The neural network is so called 'multi layer perceptron', which can be shorted as MLP. The activation function of hidden units is ReLU function. The activation function of output units is softmax function. The loss function is log loss function. We use gradient descent to optimize the parameters of the network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define three types of neurons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "ReLU(x) = \\max(0, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(-5, 5, 0.1), ReLU(np.arange(-5, 5, 0.1)), color= 'red')\n",
    "plt.ylim(-1, 6)\n",
    "plt.title('ReLU')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "softmax(x) = \\frac{e^x}{\\sum_{i=1}^n e^x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test = \n",
      "[[ 0 -1  1  1  0  1 -1  0  1 -1]]\n",
      "softmax(test) = \n",
      "[[0.06677009 0.02456334 0.18149992 0.18149992 0.06677009 0.18149992\n",
      "  0.02456334 0.06677009 0.18149992 0.02456334]]\n"
     ]
    }
   ],
   "source": [
    "test = np.random.randint(-2, 2, size= (10, 1))\n",
    "print(\"test = \")\n",
    "print(test.T)\n",
    "print(\"softmax(test) = \")\n",
    "print(softmax(test).T)\n",
    "del test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* W1: `10x784` rand matrix;\n",
    "* b1: `10x1` rand matrix;\n",
    "* W2: `10x10` rand matrix;\n",
    "* b2: `10x1` rand matrix;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot output is the real output, or the best output the neural network can achieve. For example, for the first digit in train dataset, the label is 5. So the one-hot output is $[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]^T$. The neural network will try to make the output as close as possible to the one-hot output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test = \n",
      "[1 3 5 7 9]\n",
      "one_hot(test) = \n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "test = np.array([1, 3, 5, 7, 9])\n",
    "print(\"test = \")\n",
    "print(test)\n",
    "print(\"one_hot(test) = \")\n",
    "print(one_hot(test))\n",
    "del test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the hidden layer,\n",
    "\n",
    "$$\n",
    "Z_1 = W_1 X + b_1 \\\\\n",
    "A_1 = \\mathrm{ReLU}(Z_1)\n",
    "$$\n",
    "\n",
    "For the output layer, $A_1$ is its input.\n",
    "\n",
    "$$\n",
    "Z_2 = W_2 A_1 + b_2 \\\\\n",
    "A_2 = \\mathrm{softmax}(Z_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward propagation is the most important part in neural network. Without it, neural network can't find the optimal parameters quickly. Of course, grid searching is always an option. However, it's very time consuming."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This video helps me a lot to understand the math behind it. The link is [https://www.youtube.com/watch?v=5-rVLSc2XdE](https://www.youtube.com/watch?v=5-rVLSc2XdE)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is defined as:\n",
    "\n",
    "$$\n",
    "L(\\hat{y}, y) = -\\sum_{i=0}^{c} y_i \\log(\\hat{y}_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_deriv(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "dZ_2 = A_2 - Y \\\\\n",
    "dW_2 = \\frac{1}{m} dZ_2 A_1^T \\\\\n",
    "db_2 = \\frac{1}{m} \\sum_{i=1}^m dZ_2 \\\\\n",
    "dZ_1 = W_2^T dZ_2 \\odot \\mathrm{ReLU}(Z_1) \\\\\n",
    "dW_1 = \\frac{1}{m} dZ_1 X^T \\\\\n",
    "db_1 = \\frac{1}{m} \\sum_{i=1}^m dZ_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "W_{new} = W_{old} - \\alpha dW\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ is learning rate that are determined by programmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output is a `10x1` probabilities vector. We can set the highest probability as 1, others are all set 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy evaluation function is also needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can define our training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "[9 7 0 ... 7 0 0] [5 7 9 ... 2 9 5]\n",
      "0.063953197659883\n",
      "Iteration:  10\n",
      "[9 3 4 ... 7 5 0] [5 7 9 ... 2 9 5]\n",
      "0.23571178558927947\n",
      "Iteration:  20\n",
      "[9 3 4 ... 7 5 0] [5 7 9 ... 2 9 5]\n",
      "0.34886744337216863\n",
      "Iteration:  30\n",
      "[9 7 7 ... 7 5 0] [5 7 9 ... 2 9 5]\n",
      "0.43642182109105454\n",
      "Iteration:  40\n",
      "[9 7 7 ... 7 5 0] [5 7 9 ... 2 9 5]\n",
      "0.5113755687784389\n",
      "Iteration:  50\n",
      "[9 0 7 ... 7 5 0] [5 7 9 ... 2 9 5]\n",
      "0.5750787539376969\n",
      "Iteration:  60\n",
      "[9 0 7 ... 7 5 0] [5 7 9 ... 2 9 5]\n",
      "0.6236811840592029\n",
      "Iteration:  70\n",
      "[5 0 7 ... 7 5 0] [5 7 9 ... 2 9 5]\n",
      "0.6580329016450822\n",
      "Iteration:  80\n",
      "[5 0 7 ... 8 5 0] [5 7 9 ... 2 9 5]\n",
      "0.6853842692134606\n",
      "Iteration:  90\n",
      "[5 0 7 ... 2 5 0] [5 7 9 ... 2 9 5]\n",
      "0.7058852942647132\n",
      "Iteration:  100\n",
      "[5 0 7 ... 2 5 5] [5 7 9 ... 2 9 5]\n",
      "0.7211860593029652\n",
      "Iteration:  110\n",
      "[5 0 7 ... 2 8 5] [5 7 9 ... 2 9 5]\n",
      "0.7346867343367168\n",
      "Iteration:  120\n",
      "[5 0 7 ... 2 8 5] [5 7 9 ... 2 9 5]\n",
      "0.7466373318665933\n",
      "Iteration:  130\n",
      "[5 0 7 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.7570878543927196\n",
      "Iteration:  140\n",
      "[5 0 7 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.7660383019150958\n",
      "Iteration:  150\n",
      "[5 0 7 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.7739386969348467\n",
      "Iteration:  160\n",
      "[5 0 7 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.7807890394519726\n",
      "Iteration:  170\n",
      "[5 0 7 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.7876393819690984\n",
      "Iteration:  180\n",
      "[5 0 7 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.7930396519825992\n",
      "Iteration:  190\n",
      "[5 0 7 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.7984399219960998\n",
      "Iteration:  200\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8036401820091005\n",
      "Iteration:  210\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.807940397019851\n",
      "Iteration:  220\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8127406370318516\n",
      "Iteration:  230\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.816390819540977\n",
      "Iteration:  240\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8201910095504775\n",
      "Iteration:  250\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8227411370568528\n",
      "Iteration:  260\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8257412870643532\n",
      "Iteration:  270\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8284914245712286\n",
      "Iteration:  280\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8314915745787289\n",
      "Iteration:  290\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8346917345867293\n",
      "Iteration:  300\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8374418720936047\n",
      "Iteration:  310\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.83979198959948\n",
      "Iteration:  320\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8422921146057303\n",
      "Iteration:  330\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8438421921096054\n",
      "Iteration:  340\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8459922996149808\n",
      "Iteration:  350\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8473423671183559\n",
      "Iteration:  360\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8492424621231062\n",
      "Iteration:  370\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8506925346267313\n",
      "Iteration:  380\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8520426021301065\n",
      "Iteration:  390\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8533426671333567\n",
      "Iteration:  400\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8541427071353568\n",
      "Iteration:  410\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.856242812140607\n",
      "Iteration:  420\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8577928896444822\n",
      "Iteration:  430\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8592929646482325\n",
      "Iteration:  440\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8604930246512326\n",
      "Iteration:  450\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8615930796539827\n",
      "Iteration:  460\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8625931296564828\n",
      "Iteration:  470\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8633431671583579\n",
      "Iteration:  480\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8647432371618581\n",
      "Iteration:  490\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8655932796639833\n",
      "Iteration:  500\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8669433471673583\n",
      "Iteration:  510\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8680434021701086\n",
      "Iteration:  520\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8690434521726086\n",
      "Iteration:  530\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8696934846742337\n",
      "Iteration:  540\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.871193559677984\n",
      "Iteration:  550\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8723936196809841\n",
      "Iteration:  560\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8734936746837342\n",
      "Iteration:  570\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8739936996849843\n",
      "Iteration:  580\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8747437371868594\n",
      "Iteration:  590\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8751937596879844\n",
      "Iteration:  600\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8761438071903596\n",
      "Iteration:  610\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8765938296914846\n",
      "Iteration:  620\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8775438771938597\n",
      "Iteration:  630\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8782939146957348\n",
      "Iteration:  640\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8789439471973599\n",
      "Iteration:  650\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.87964398219911\n",
      "Iteration:  660\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.879993999699985\n",
      "Iteration:  670\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8804440222011101\n",
      "Iteration:  680\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.880594029701485\n",
      "Iteration:  690\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8810940547027352\n",
      "Iteration:  700\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8816940847042352\n",
      "Iteration:  710\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8821441072053603\n",
      "Iteration:  720\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8824941247062353\n",
      "Iteration:  730\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8830441522076103\n",
      "Iteration:  740\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8836941847092354\n",
      "Iteration:  750\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8843442172108605\n",
      "Iteration:  760\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8849942497124856\n",
      "Iteration:  770\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8853942697134857\n",
      "Iteration:  780\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8859442972148608\n",
      "Iteration:  790\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8863943197159858\n",
      "Iteration:  800\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8867443372168609\n",
      "Iteration:  810\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8872443622181109\n",
      "Iteration:  820\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.887694384719236\n",
      "Iteration:  830\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.887794389719486\n",
      "Iteration:  840\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.888194409720486\n",
      "Iteration:  850\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8885444272213611\n",
      "Iteration:  860\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.888694434721736\n",
      "Iteration:  870\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8890944547227362\n",
      "Iteration:  880\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8892444622231112\n",
      "Iteration:  890\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8898944947247363\n",
      "Iteration:  900\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8900945047252362\n",
      "Iteration:  910\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8905945297264863\n",
      "Iteration:  920\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8911445572278613\n",
      "Iteration:  930\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8911945597279864\n",
      "Iteration:  940\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8913445672283614\n",
      "Iteration:  950\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8914445722286114\n",
      "Iteration:  960\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8917445872293615\n",
      "Iteration:  970\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8922446122306116\n",
      "Iteration:  980\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8925446272313615\n",
      "Iteration:  990\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8928446422321116\n",
      "Iteration:  1000\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8929946497324867\n",
      "Iteration:  1010\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8931946597329866\n",
      "Iteration:  1020\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8933446672333617\n",
      "Iteration:  1030\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8934946747337367\n",
      "Iteration:  1040\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8935446772338617\n",
      "Iteration:  1050\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8937446872343617\n",
      "Iteration:  1060\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8941447072353618\n",
      "Iteration:  1070\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8942947147357367\n",
      "Iteration:  1080\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8946947347367369\n",
      "Iteration:  1090\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8950447522376119\n",
      "Iteration:  1100\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.895394769738487\n",
      "Iteration:  1110\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8955447772388619\n",
      "Iteration:  1120\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.895794789739487\n",
      "Iteration:  1130\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.895994799739987\n",
      "Iteration:  1140\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.896294814740737\n",
      "Iteration:  1150\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8965448272413621\n",
      "Iteration:  1160\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8968448422421121\n",
      "Iteration:  1170\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8969448472423621\n",
      "Iteration:  1180\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8971948597429872\n",
      "Iteration:  1190\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8973448672433622\n",
      "Iteration:  1200\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8974448722436121\n",
      "Iteration:  1210\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8977448872443622\n",
      "Iteration:  1220\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8979948997449873\n",
      "Iteration:  1230\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8982449122456123\n",
      "Iteration:  1240\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8983449172458623\n",
      "Iteration:  1250\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8983949197459873\n",
      "Iteration:  1260\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8983949197459873\n",
      "Iteration:  1270\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8986949347467373\n",
      "Iteration:  1280\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8987449372468623\n",
      "Iteration:  1290\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8989949497474874\n",
      "Iteration:  1300\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8991449572478624\n",
      "Iteration:  1310\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8992449622481125\n",
      "Iteration:  1320\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.8993949697484874\n",
      "Iteration:  1330\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9000450022501125\n",
      "Iteration:  1340\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9002450122506125\n",
      "Iteration:  1350\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9003450172508626\n",
      "Iteration:  1360\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9005950297514875\n",
      "Iteration:  1370\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9007450372518626\n",
      "Iteration:  1380\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9009450472523626\n",
      "Iteration:  1390\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9012450622531126\n",
      "Iteration:  1400\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9012950647532376\n",
      "Iteration:  1410\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9014950747537377\n",
      "Iteration:  1420\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9015950797539877\n",
      "Iteration:  1430\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9017450872543628\n",
      "Iteration:  1440\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9018450922546127\n",
      "Iteration:  1450\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9018450922546127\n",
      "Iteration:  1460\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9020451022551128\n",
      "Iteration:  1470\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9020451022551128\n",
      "Iteration:  1480\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9022451122556128\n",
      "Iteration:  1490\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9024451222561128\n",
      "Iteration:  1500\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9025451272563628\n",
      "Iteration:  1510\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9027951397569879\n",
      "Iteration:  1520\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9030451522576128\n",
      "Iteration:  1530\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9031951597579879\n",
      "Iteration:  1540\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.903445172258613\n",
      "Iteration:  1550\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.903845192259613\n",
      "Iteration:  1560\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.903995199759988\n",
      "Iteration:  1570\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.904045202260113\n",
      "Iteration:  1580\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.904245212260613\n",
      "Iteration:  1590\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9044952247612381\n",
      "Iteration:  1600\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.904395219760988\n",
      "Iteration:  1610\n",
      "[5 0 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9044952247612381\n",
      "Iteration:  1620\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9048452422621132\n",
      "Iteration:  1630\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9050952547627381\n",
      "Iteration:  1640\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9051952597629882\n",
      "Iteration:  1650\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9054452722636132\n",
      "Iteration:  1660\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9057452872643632\n",
      "Iteration:  1670\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9057952897644882\n",
      "Iteration:  1680\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9060453022651133\n",
      "Iteration:  1690\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9062453122656133\n",
      "Iteration:  1700\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9062453122656133\n",
      "Iteration:  1710\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9062453122656133\n",
      "Iteration:  1720\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9064453222661133\n",
      "Iteration:  1730\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9064953247662383\n",
      "Iteration:  1740\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9065953297664884\n",
      "Iteration:  1750\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9068953447672383\n",
      "Iteration:  1760\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9068953447672383\n",
      "Iteration:  1770\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9069953497674884\n",
      "Iteration:  1780\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9070953547677384\n",
      "Iteration:  1790\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9072453622681134\n",
      "Iteration:  1800\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9074953747687384\n",
      "Iteration:  1810\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9075953797689884\n",
      "Iteration:  1820\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9077453872693635\n",
      "Iteration:  1830\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9080954047702385\n",
      "Iteration:  1840\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9082954147707385\n",
      "Iteration:  1850\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9083954197709886\n",
      "Iteration:  1860\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9085454272713636\n",
      "Iteration:  1870\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9085954297714885\n",
      "Iteration:  1880\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9086954347717386\n",
      "Iteration:  1890\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9088454422721136\n",
      "Iteration:  1900\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9089454472723636\n",
      "Iteration:  1910\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9090454522726137\n",
      "Iteration:  1920\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9091954597729887\n",
      "Iteration:  1930\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9092454622731136\n",
      "Iteration:  1940\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9094954747737387\n",
      "Iteration:  1950\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9096954847742387\n",
      "Iteration:  1960\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9098454922746138\n",
      "Iteration:  1970\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9099454972748637\n",
      "Iteration:  1980\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9101955097754888\n",
      "Iteration:  1990\n",
      "[5 7 9 ... 2 9 5] [5 7 9 ... 2 9 5]\n",
      "0.9103955197759888\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.2, 2000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    predictions = get_predictions(A2)\n",
    "    return predictions\n",
    "\n",
    "Y_pred = make_predictions(X_test, W1, b1, W2, b2)\n",
    "\n",
    "Y_pred.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 ... 4 5 6] [2 1 0 ... 4 5 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90.45904590459047"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = get_accuracy(Y_pred, Y_test) * 100\n",
    "accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, our model has 90.5% accuracy on test set. Nice result!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
